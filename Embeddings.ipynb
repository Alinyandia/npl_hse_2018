{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from lxml import html\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.decomposition import TruncatedSVD, NMF, PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import gensim\n",
    "import numpy as np\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from collections import Counter,defaultdict\n",
    "from string import punctuation\n",
    "import os\n",
    "from nltk.corpus import stopwords\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "%matplotlib inline\n",
    "\n",
    "morph = MorphAnalyzer()\n",
    "punct = punctuation+'«»—…“”*№–'\n",
    "stops = set(stopwords.words('russian'))\n",
    "\n",
    "def normalize(text):\n",
    "    \n",
    "    words = [word.strip(punct) for word in text.lower().split()]\n",
    "    words = [morph.parse(word)[0].normal_form for word in words if word and word not in stops]\n",
    "\n",
    "    return ' '.join(words)\n",
    "\n",
    "def tokenize(text):\n",
    "    \n",
    "    words = [word.strip(punct) for word in text.lower().split()]\n",
    "\n",
    "    return ' '.join(words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для обучения векторных представлений необходимо большое количество текста. Чем больше текста, тем лучше предтавления получатся.  \n",
    "Возьмем ~7к новостных статей. Это все ещё маленький корпус, но для обучения он подходит (на нем можно достаточно быстро попробовать разные методы). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_rt = pd.read_csv('/Users/alinashaymardanova/Downloads/news_texts.csv')\n",
    "data_rt.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>content_norm</th>\n",
       "      <th>tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Канцлер Германии Ангела Меркель в ходе брифинг...</td>\n",
       "      <td>[канцлер, германия, ангел, меркель, ход, брифи...</td>\n",
       "      <td>канцлер германии ангела меркель в ходе брифинг...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Российские и белорусские войска успешно заверш...</td>\n",
       "      <td>[российский, белорусский, войско, успешно, зав...</td>\n",
       "      <td>российские и белорусские войска успешно заверш...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Дзюба, Шатов и Анюков оказались не нужны «Зени...</td>\n",
       "      <td>[дзюба, шат, анюк, оказаться, нужный, зенит, р...</td>\n",
       "      <td>дзюба шатов и анюков оказались не нужны зениту...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>В Испанию без фанатов\\nПожалуй, главной пятнич...</td>\n",
       "      <td>[испания, фанат, пожалуй, главный, пятничный, ...</td>\n",
       "      <td>в испанию без фанатов пожалуй главной пятнично...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Постпред России при ООН Виталий Чуркин, говоря...</td>\n",
       "      <td>[постпред, россия, оон, виталий, чуркин, говор...</td>\n",
       "      <td>постпред россии при оон виталий чуркин говоря ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content  \\\n",
       "0  Канцлер Германии Ангела Меркель в ходе брифинг...   \n",
       "1  Российские и белорусские войска успешно заверш...   \n",
       "2  Дзюба, Шатов и Анюков оказались не нужны «Зени...   \n",
       "3  В Испанию без фанатов\\nПожалуй, главной пятнич...   \n",
       "4  Постпред России при ООН Виталий Чуркин, говоря...   \n",
       "\n",
       "                                        content_norm  \\\n",
       "0  [канцлер, германия, ангел, меркель, ход, брифи...   \n",
       "1  [российский, белорусский, войско, успешно, зав...   \n",
       "2  [дзюба, шат, анюк, оказаться, нужный, зенит, р...   \n",
       "3  [испания, фанат, пожалуй, главный, пятничный, ...   \n",
       "4  [постпред, россия, оон, виталий, чуркин, говор...   \n",
       "\n",
       "                                           tokenized  \n",
       "0  канцлер германии ангела меркель в ходе брифинг...  \n",
       "1  российские и белорусские войска успешно заверш...  \n",
       "2  дзюба шатов и анюков оказались не нужны зениту...  \n",
       "3  в испанию без фанатов пожалуй главной пятнично...  \n",
       "4  постпред россии при оон виталий чуркин говоря ...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_rt['content_norm'] = data_rt['content_norm'].apply(str.split)\n",
    "data_rt['tokenized'] = data_rt['content'].apply(tokenize)\n",
    "data_rt.to_csv('news_texts.tsv', sep='\\t', index=None)\n",
    "data_rt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_xml = html.fromstring(open('/Users/alinashaymardanova/Downloads/paraphraser/paraphrases.xml', 'rb').read())\n",
    "texts_1 = []\n",
    "texts_2 = []\n",
    "classes = []\n",
    "\n",
    "for p in corpus_xml.xpath('//paraphrase'):\n",
    "    texts_1.append(p.xpath('./value[@name=\"text_1\"]/text()')[0])\n",
    "    texts_2.append(p.xpath('./value[@name=\"text_2\"]/text()')[0])\n",
    "    classes.append(p.xpath('./value[@name=\"class\"]/text()')[0])\n",
    "    \n",
    "data = pd.DataFrame({'text_1':texts_1, 'text_2':texts_2, 'label':classes})\n",
    "\n",
    "data['text_1_norm'] = data['text_1'].apply(normalize)\n",
    "data['text_2_norm'] = data['text_2'].apply(normalize)\n",
    "data['text_1_tokenized'] = data['text_1'].apply(tokenize)\n",
    "data['text_2_tokenized'] = data['text_2'].apply(tokenize)\n",
    "\n",
    "data.to_csv('paraphrases.tsv', sep='\\t', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_rt = pd.read_csv('news_texts.tsv', sep='\\t')\n",
    "data = pd.read_csv('paraphrases.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_embedding(text, model, dim, n_documents=None, inv_idx=None):\n",
    "    words = Counter(text)\n",
    "    total = len(text)\n",
    "    vectors = np.zeros((len(words), dim))\n",
    "    \n",
    "    for i, word in enumerate(words):\n",
    "        try:\n",
    "            v = model.wv[word]\n",
    "            if inv_idx:\n",
    "                vectors[i] = v * (words[word] / total) * log(n_documents / inv_idx[word])\n",
    "            else:\n",
    "                vectors[i] = v\n",
    "        except (KeyError, ValueError):\n",
    "            continue\n",
    "    \n",
    "    if vectors.any():\n",
    "        vector = np.average(vectors, axis=0)\n",
    "    else:\n",
    "        vector = np.zeros((dim))\n",
    "    \n",
    "    return vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity(v1, v2):\n",
    "    v1_norm = gensim.matutils.unitvec(np.array(v1))\n",
    "    v2_norm = gensim.matutils.unitvec(np.array(v2))\n",
    "    return np.dot(v1_norm, v2_norm)\n",
    "\n",
    "def transformer(data, model, dim, inv_idx=None):\n",
    "    n_documents = len(data)\n",
    "    X_text = np.zeros((n_documents, dim))\n",
    "                \n",
    "    for i, text in enumerate(data):\n",
    "        X_text[i] = get_embedding(text, model, dim, n_documents * 2, inv_idx)\n",
    "    \n",
    "    return X_text\n",
    "\n",
    "def get_similarity(model, data_1, data_2, dim, vect=None, embeddings_needed=False, weighted_tfidf=False, tokenized=False):\n",
    "\n",
    "    if embeddings_needed:\n",
    "        \n",
    "        if weighted_tfidf:\n",
    "            if tokenized:\n",
    "                X_text_1 = transformer(data_1.values, model, dim, inv_idx_tokenized)\n",
    "                X_text_2 = transformer(data_2.values, model, dim, inv_idx_tokenized)\n",
    "            else:\n",
    "                X_text_1 = transformer(data_1.values, model, dim, inv_idx)\n",
    "                X_text_2 = transformer(data_2.values, model, dim, inv_idx)\n",
    "        else:    \n",
    "            X_text_1 = transformer(data_1.values, model, dim)\n",
    "            X_text_2 = transformer(data_2.values, model, dim)\n",
    "    else:\n",
    "        \n",
    "        X_text_1 = model.transform(vect.transform(data_1))\n",
    "        X_text_2 = model.transform(vect.transform(data_2))\n",
    "        \n",
    "    sim = [similarity(v1, v2) for v1, v2 in zip(X_text_1, X_text_2)]\n",
    "    return sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = CountVectorizer(min_df=3, max_df=0.4, max_features=1000, lowercase=False, tokenizer=lambda x: x)\n",
    "X_count = count.fit_transform(data_rt['content_norm'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NMF(alpha=0.0, beta_loss='frobenius', init=None, l1_ratio=0.0, max_iter=200,\n",
       "  n_components=50, random_state=None, shuffle=False, solver='cd',\n",
       "  tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dim = 50\n",
    "nmf_ = NMF(dim)\n",
    "nmf_.fit(X_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "r_nmf = get_similarity(nmf_, data['text_1_norm'], data['text_2_norm'], dim, vect=count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(min_df=3, max_df=0.4, max_features=1000, lowercase=False, tokenizer=lambda x: x)\n",
    "X_tfidf = tfidf.fit_transform(data_rt['content_norm'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NMF(alpha=0.0, beta_loss='frobenius', init=None, l1_ratio=0.0, max_iter=200,\n",
       "  n_components=50, random_state=None, shuffle=False, solver='cd',\n",
       "  tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nmf_tfidf = NMF(dim)\n",
    "nmf_tfidf.fit(X_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "r_nmf_tfidf = get_similarity(nmf_tfidf, data['text_1_norm'], data['text_2_norm'], dim, vect=tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TruncatedSVD(algorithm='randomized', n_components=50, n_iter=5,\n",
       "       random_state=None, tol=0.0)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svd_ = TruncatedSVD(dim)\n",
    "svd_.fit(X_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "r_svd_count = get_similarity(svd_, data['text_1_norm'], data['text_2_norm'], dim, vect=count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TruncatedSVD(algorithm='randomized', n_components=50, n_iter=5,\n",
       "       random_state=None, tol=0.0)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svd_tfidf = TruncatedSVD(dim)\n",
    "svd_tfidf.fit(X_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "r_svd_tfidf = get_similarity(svd_tfidf, data['text_1_norm'], data['text_2_norm'], dim, vect=tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fastext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ne norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from math import log\n",
    "\n",
    "def idx(data):\n",
    "    inverted_index = defaultdict(list)\n",
    "    for i, doc in enumerate(data):\n",
    "        for word in doc: \n",
    "            inverted_index[word].append(i)\n",
    "    \n",
    "    inv_idx = {word:len(inverted_index[word]) for word in inverted_index}\n",
    "    return inv_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_idx = idx(np.concatenate([data['text_1_norm'], data['text_2_norm']], axis=0))\n",
    "inv_idx_tokenized = idx(np.concatenate([data['text_1_tokenized'], data['text_1_tokenized']], axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft = gensim.models.FastText(data_rt['tokenized'], size=dim, min_n=4, max_n=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "res_ft = get_similarity(ft, data['text_1_tokenized'], data['text_2_tokenized'], dim, embeddings_needed=True)\n",
    "res_ft_tfidf = get_similarity(ft, data['text_1_tokenized'], data['text_2_tokenized'], dim, embeddings_needed=True, weighted_tfidf=True, tokenized=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_norm = gensim.models.FastText(data_rt['content_norm'], size=50, min_n=4, max_n=8)\n",
    "r_ft_norm = get_similarity(ft_norm, data['text_1_norm'], data['text_2_norm'], dim, embeddings_needed=True)\n",
    "res_ft_norm_tfidf = get_similarity(ft_norm, data['text_1_norm'], data['text_2_norm'], dim, embeddings_needed=True, weighted_tfidf=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w2v = gensim.models.Word2Vec(data_rt['content_norm'], size=dim, sg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "r_w2v = get_similarity(w2v, data['text_1_norm'], data['text_2_norm'], dim, embeddings_needed=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "r_w2v_tfidf = get_similarity(w2v, data['text_1_norm'], data['text_2_norm'], dim, embeddings_needed=True, weighted_tfidf=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Объединим"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.DataFrame({'nmf': r_nmf, 'nmf_tfidf': r_nmf_tfidf,\n",
    "                    'svd': r_svd_count, 'svd_tfidf': r_svd_tfidf,\n",
    "                    'ft': res_ft, 'ft_tfidf': res_ft_tfidf,\n",
    "                    'ft_norm': r_ft_norm, 'ft_norm_tfidf': res_ft_norm_tfidf,\n",
    "                    'w2v': r_w2v, 'w2v_tfidf': res_w2v_tfidf})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.44498552718470491"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lg = LogisticRegression(class_weight='balanced')\n",
    "cross_val_score(lg, result, data['label'], cv=5, scoring='f1_micro', n_jobs=-1).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.48870008930535996"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestClassifier(n_estimators=500, class_weight='balanced')\n",
    "cross_val_score(rf, result, data['label'], cv=5, scoring='f1_micro', n_jobs=-1).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Попробуем \"поиграть\" со значениями w2v, SVD и NMF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### w2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w2v = gensim.models.Word2Vec(data_rt['content_norm'], size=100, sg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "r_w2v = get_similarity(w2v, data['text_1_norm'], data['text_2_norm'], dim, embeddings_needed=True)\n",
    "r_w2v_tfidf = get_similarity(w2v, data['text_1_norm'], data['text_2_norm'], dim, embeddings_needed=True, weighted_tfidf=True)\n",
    "result.update(pd.DataFrame({'w2v': r_w2v, \n",
    "                            'w2v_tfidf': r_w2v_tfidf}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.44540218826781314"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(LogisticRegression(class_weight='balanced'), result, data['label'], cv=5, scoring='f1_micro', n_jobs=-1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.46629170672228132"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(RandomForestClassifier(n_estimators=500, class_weight='balanced'), result, data['label'], cv=5, scoring='f1_micro', n_jobs=-1).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### svd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd_ = TruncatedSVD(100)\n",
    "svd_.fit(X_count)\n",
    "r_svd = get_similarity(svd_, data['text_1_norm'], data['text_2_norm'], 100, vect=count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "svd_tfidf = TruncatedSVD(100)\n",
    "svd_tfidf.fit(X_tfidf)\n",
    "r_svd_tfidf = get_similarity(svd_tfidf, data['text_1_norm'], data['text_2_norm'], 100, vect=tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.update(pd.DataFrame({'svd_count': r_svd, \n",
    "                            'svd_tfidf': r_svd_tfidf_second}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.44540218826781314"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(LogisticRegression(class_weight='balanced'), result, data['label'], cv=5, scoring='f1_micro', n_jobs=-1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4676734945190234"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(RandomForestClassifier(n_estimators=500, class_weight='balanced'), result, data['label'], cv=5, scoring='f1_micro', n_jobs=-1).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### nmf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nmf_ = NMF(100)\n",
    "nmf_.fit(X_count)\n",
    "r_nmf = get_similarity(nmf_, data['text_1_norm'], data['text_2_norm'], 100, vect=count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nmf_tfidf = NMF(100)\n",
    "nmf_tfidf.fit(X_tfidf)\n",
    "r_nmf_tfidf = get_similarity(nmf_tfidf, data['text_1_norm'], data['text_2_norm'], 100, vect=tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.update(pd.DataFrame({'nmf_count': nmf_, \n",
    "                                'nmf_tfidf': r_nmf_tfidf}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.44540218826781314"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(LogisticRegression(class_weight='balanced'), result, data['label'], cv=5, scoring='f1_micro', n_jobs=-1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.46988984775800591"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(RandomForestClassifier(n_estimators=500, class_weight='balanced'), result, data['label'], cv=5, scoring='f1_micro', n_jobs=-1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
